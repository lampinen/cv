% LaTeX file for resume 
% This file uses the resume document class (res.cls)

\documentclass[margin]{res} 
% the margin option causes section titles to appear to the left of body text 
\textwidth=5.2in % increase textwidth to get smaller right margin
%\usepackage{helvetica} % uses helvetica postscript font (download helvetica.sty)
%\usepackage{newcent}   % uses new century schoolbook postscript font 
\usepackage{url}
\begin{document} 
 
\name{Andrew Kyle Lampinen\\[14pt]}
 
\address{{\bf Email} \\ andrewlampinen@gmail.com}
\address{{\bf Website} \\\url{https://lampinen.github.io}}
\begin{resume} 
\section{Education} 
{\bf Stanford University,} Ph.D. Psychology (Cognitive), 2015-2020
\begin{itemize} \itemsep -2pt \item Center for Mind, Brain, Computation, and Technology Trainee. \item Minor in Computer Science.\end{itemize}
{\bf UC Berkeley,}  B.A. Mathematics \& Physics, 2010-2014%\begin{itemize} \itemsep -2pt \item Highest honors in mathematics, high distinction in general scholarship.% \item GPA: 4.0 Math, 3.9 Physics, 3.9 cumulative. %\item Study Abroad Internship, A*STAR IHPC Singapore, Summer 2012. (See Research Experience.)
%\end{itemize}
\vspace{1pt}\section{Research\\Positions} \vspace{-15pt} \rule{\textwidth}{0.5pt} \\[3pt]
{\bf Staff Research Scientist,} Google DeepMind, May 2024 - Present\\[0.1em] 
{\bf Senior Research Scientist,} Google DeepMind, March 2022 - April 2024\\[0.1em] 
{\bf Research Scientist,} DeepMind, October 2020 - February 2022\\[0.1em]
{\bf PhD Intern,} DeepMind, May 2019 - September 2019\\[0.1em]
{\bf PhD Software Engineering Intern,} Google Brain, June 2017 - September 2017\\[0.1em]
{\bf Associate Professional Staff I,} Johns Hopkins University Applied Physics Laboratory, June 2014 - July 2015\\[0.1em] 
{\bf Student Research Associate,} Lawrence Berkeley National Laboratories, January - May 2012 \& August - December 2012\\[0.1em]
{\bf Summer Research Intern,} A*STAR Institute of High Performance Computing, Singapore, June - August 2012\\[0.1em]
{\bf Research Assistant,} UC Davis Plant Sciences, June - August 2011
\vspace{1pt}
\section{Honors} \vspace{-15pt} \rule{\textwidth}{0.5pt} \\[3pt]
Cognitive Science Society Robert J. Glushko Dissertation Prize, 2021 \\
Ric Weiland Graduate Fellowship in the Humanities and Sciences, 2018-2020 \\
National Science Foundation Graduate Research Fellowship, 2015-2018 \\
Percy Lionel Davis Award for Excellence in Scholarship in Mathematics, 2014 \\ 
Berkeley Physics Olsen Scholar 2013-2014 \\
Berkeley Letters \& Science Dean's List 2012-2014\\
Berkeley Physics Undergraduate Research Scholar, Spring \& Fall 2012
%\vspace{0pt}
\vspace{1pt}\section{Selected Publications} \vspace{-15pt} \rule{\textwidth}{0.5pt} \\[3pt]
\textbf{Andrew K. Lampinen}, Stephanie C. Y. Chan, Katherine Hermann (2024), {``Learned feature representations are biased by complexity, learning order, position, and more'',} \textit{Transactions on Machine Learning Research} \\[3pt]
\textbf{Andrew K. Lampinen} (2024), {``Can language models handle recursively nested grammatical structures? A case study on comparing models and humans'',} \textit{Computational Linguistics} \\[3pt]
\textbf{Andrew Lampinen*}, Ishita Dasgupta*, Stephanie Chan, Hannah Sheahan, Antonia Creswell, Dharshan Kumaran, James L. McClleland, Felix Hill (2024), {``Language models, like humans, show content effects on reasoning tasks'',} \textit{PNAS Nexus}, (*equal contribution) \\[3pt] 
Thomas Fel, Louis B{\'e}thune, \textbf{Andrew K. Lampinen}, Thomas Serre,  Katherine Hermann (2024), {``Understanding Visual Feature Reliance through the Lens of Complexity'',} \textit{Advances in Neural Information Processing Systems} \\[3pt]
Dan Friedman, \textbf{Andrew Lampinen}, Lucas Dixon, Danqi Chen, Asma Ghandeharioun (2024), {``Interpretability illusions in the generalization of simplified models'',} \textit{International Conference on Machine Learning} \\[3pt]
Drew A Hudson, Daniel Zoran, Mateusz Malinowski, \textbf{Andrew K. Lampinen}, Andrew Jaegle, James L. McClelland, Loic Matthey, Felix Hill, Alexander Lerchner (2023), {``SODA: Bottleneck Diffusion Models for Representation Learning''}, \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition} \\[3pt]
\textbf{Andrew K. Lampinen}, Stephanie C. Y. Chan, Ishita Dasgupta, Andrew J. Nam, Jane X. Wang (2023), {``Passive learning of active causal strategies in agents and language models'',} \textit{Advances in Neural Information Processing Systems} \\[3pt]
Lukas Muttenthaler, Lorenz Linhardt, Jonas Dippel, Robert A. Vandermeulen, Katherine Hermann, \textbf{Andrew K. Lampinen}, Simon Kornblith (2023), {``Improving neural network representations using human similarity judgements'',} \textit{Advances in Neural Information Processing Systems} \\[3pt]
Wilka Carvalho, Andre Saraiva, Angelos Filos, \textbf{Andrew K. Lampinen}, Loic Matthey, Richard L. Lewis, Honglak Lee, Satinder Singh, Danilo J Rezende, Daniel Zoran (2023), {``Combining behaviors with the successor features keyboard''}, \textit{Advances in Neural Information Processing Systems} \\[3pt]
Jerry Wei, Le Hou, \textbf{Andrew K. Lampinen}, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, Quoc V. Le (2023), {``Symbol tuning improves in-context learning in language models'',} \textit{Proceedings of Empirical Methods in Natural Language Processing} \\[3pt]
Aaditya K. Singh, David Ding, Andrew Saxe, Felix Hill, \textbf{Andrew K. Lampinen} (2023), {``Know your audience: Specializing grounded language models with the game of Dixit'',} \textit{Proceedings of the European Chapter of the Association for Computational Linguistics} \\[3pt]
\textbf{Andrew K. Lampinen}, Ishita Dasgupta, Stephanie C. Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, Felix Hill (2022), {``Can language models learn from explanations in context?'',} \textit{Findings of Empiricical Methods in Natural Language Processing} \\[3pt] 
Allison C. Tam,  Neil. C. Rabinowitz, \textbf{Andrew K. Lampinen}, Nick A. Roy, Stephanie C. Y. Chan, DJ Strouse, Jane X. Wang, Andrea Banino, Felix Hill (2022), {``Semantic exploration from language abstractions and pretrained representations'',} \textit{Advances in Neural Information Processing Systems} \\[3pt] 
Stephanie C. Y. Chan, Adam Santoro, \textbf{Andrew K. Lampinen}, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, James L. McClelland, Felix Hill (2022), {``Data Distributional Properties Drive Emergent In-Context Learning in Transformers'',} \textit{Advances in Neural Information Processing Systems} \\[3pt] 
Stephanie C. Y. Chan, Ishita Dasgputa, Junkyung Kim, Dharshan Kumaran, \textbf{Andrew K. Lampinen}, Felix Hill (2022), {``Transformers generalize diffierently from information stored in context vs in weights'',} \textit{Memory in Artificial \& Natural Intelligence Workshop, NeurIPS 2022} \\[3pt]
\textbf{Andrew K. Lampinen}, Nicholas A. Roy, Ishita Dasgupta, Stephanie C. Y. Chan, Allison C. Tam, James L. McClelland, Chen Yan, Adam Santoro, Neil C. Rabinowitz, Jane X. Wang, Felix Hill (2022), {``Tell me why!---Explanations support learning of relational and causal structure'',} \textit{International Conference on Machine Learning} \\[3pt] 
Stephanie C. Y. Chan*, \textbf{Andrew K. Lampinen}*, Pierre H. Richemond*, Felix Hill* (2022), {``Zipfian Environments for Reinforcement Learning'',} \textit{Conference on Lifelong Learning Agents}, (*equal contribution) \\[3pt] 
\textbf{Andrew K. Lampinen}, Stephanie C. Y. Chan, Andrea Banino, Felix Hill (2021), {``Towards mental time travel: a hierarchical memory for reinforcement learning agents'',} \textit{Advances in Neural Information Processing Systems} \\[3pt] 
\textbf{Andrew K. Lampinen}, Stephanie C. Y. Chan, Adam Santoro, Felix Hill, (2021), {``Publishing fast and slow: A path towards generalizability in psychology and AI'',} Commentary in \textit{Behavioral and Brain Sciences} \\[3pt] 
\textbf{Andrew K. Lampinen} and James L. McClelland, (2020), {``Transforming task representations to perform novel tasks'',} \textit{Proceedings of the National Academy of Sciences} \\[3pt] 
Katherine L. Hermann* and \textbf{Andrew K. Lampinen}*, (2020), {``What shapes feature representations? Exploring datasets, architectures, and training'',} \textit{Advances in Neural Information Processing Systems}, (*equal contribution) \\[3pt] 
James L. McClelland, Bruce L. McNaughton, and \textbf{Andrew K. Lampinen} (2020), {``Integration of new information in memory: new insights from a complementary learning sytems perspective''}, \textit{Proceedings of the Royal Society B} \\[3pt]
S\'ebastien Racani\`ere*, \textbf{Andrew K. Lampinen}*, Adam Santoro, David P. Reichert, Vlad Firoiu, and Timothy P. Lillicrap, (2020), {``Automated curricula through setter-solver interactions'',} \textit{Proceedings of the 8th International Conference on Learning Representations}, (*equal contribution) \\ [3pt] 
Felix Hill, \textbf{Andrew K. Lampinen}, Rosalia Schneider, Stephen Clark, Matthew Bot-vinick, James L. McClelland, and Adam Santoro (2020), {``Environmental drivers of systematicity and generalisation in a situated agent'',} \textit{Proceedings of the 8th International Conference on Learning Representations} \\ [3pt] 
\textbf{Andrew K. Lampinen} and James L. McClelland, (2019), {``Zero-shot task adaptation by homoiconic meta-mapping'',} \textit{Learning Transferable Skills Workshop, NeurIPS} \\ [3pt] 
\textbf{Andrew K. Lampinen} and Surya Ganguli, (2019), {``An analytic theory of generalization dynamics and transfer learning in deep linear networks'',} \textit{Proceedings of the 7th International Conference on Learning Representations} \\[3pt] 
\textbf{Andrew K. Lampinen} and James L. McClelland, (2018), {``Different presentations of a mathematical concept can support learning in complementary ways'',} \textit{Journal of Educational Psychology} \\[3pt]
 Robert X. D. Hawkins, Eric N. Smith, Carolyn Au, Juan Miguel Arias, Rhia Catapano, Eric Hermann, Martin Keil, \textbf{Andrew Lampinen}, Sarah Raposo, Jesse Reynolds, Shima Salehi, Justin Salloum, Jed Tan, and Michael C. Frank, (2018), {``Improving the replicability of Psychological Science through pedagogy'',}  \textit{Advances in Methods and Practices in Psychological Science} \\ [3pt]
Steven S. Hansen, \textbf{Andrew K. Lampinen}, Gaurav Suri, and James L. McClelland, (2017), {``Building on prior knowledge without building it in'',} \textit{Commentary in Behavioral \& Brain Sciences}  \\[3pt]
\textbf{Andrew K. Lampinen}, Shaw Hsu, and James L. McClelland, (2017), {``Analogies emerge from learning dynamics in neural networks'',} \textit{Proceedings of the 39th Annual Meeting of the Cognitive Science Society}  

\vspace{1pt}\section{Selected\phantom{blah} Preprints} \vspace{-15pt} \rule{\textwidth}{0.5pt} \\[3pt]
Lukas Muttenthaler, Klaus Greff, Frieda Born, Bernhard Spitzer, Simon Kornblith, Michael C. Mozer, Klaus-Robert M{\"u}ller, \textbf{Andrew K. Lampinen}, (2024), {``Aligning machine and human visual representations across abstraction levels'',} \textit{arXiv}\\[3pt] 
Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C Love, Erin Grant, Jascha Achterberg, Joshua B Tenenbaum, Katherine M Collins, Katherine L Hermann, Kerem Oktar, Klaus Greff, Martin N Hebart, Nori Jacoby, Raja Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle, Thomas P O'Connell, Thomas Unterthiner, \textbf{Andrew K. Lampinen*}, Klaus-Robert M{\"u}ller*, Mariya Toneva*, Thomas L Griffiths* (2023), {``Getting aligned on representational alignment'',} \textit{arXiv}, (*equal advising/senior authors) \\[3pt]
Adam Santoro*, \textbf{Andrew K. Lampinen*}, Kory Mathewson, Timothy Lillicrap, David Raposo, (2021), {``Symbolic Behaviour in Artificial Intelligence'',} \textit{arXiv}, (*equal contribution) \\[3pt] 
\textbf{Andrew K. Lampinen} and James L. McClelland, (2017), {``One-shot and few-shot learning of word embeddings'',} \textit{arXiv} 

\vspace{1pt}\section{Selected Invited Talks} \vspace{-15pt} \rule{\textwidth}{0.5pt} \\[3pt]
{``A rose by any other representation: some questions on the relationship between representation \& computation'',} \textit{Princeton Neuroscience Institute}, August 2024\\[3pt] 
{``Symbolic behaviour in AI: some possible lessons for whales?'',} \textit{Decoding Communication in Nonhuman Species Workshop, Simons Institute for the Theory of Computing}, June 2024\\[3pt]
{``What can be passively learned about causality?'',} \textit{Understanding Higher-Level Intelligence Workshop, Simons Institute for the Theory of Computing}, June 2024
{``Improving neural network representations by aligning with human knowledge'',} \textit{Representational Alignment Workshop, ICLR 2024}, May 2024\\[3pt] 
{``What could language models learn about causality?'',} \textit{Large Language Models and Cognitive Science Seminar, UC Berkeley}, April 2024\\[3pt] 
{``Comparing humans and language models: reasoning \& grammar'',} \textit{Cognition, Brain \& Behavior Seminar, Harvard University}, February 2024\\[3pt] 
{``A rose by any other representation: some questions on the relationship between representation \& computation'',} \textit{NeurIPS UniReps Workshop}, December 2023\\[3pt] 
{``Comparing humans and language models: reasoning \& grammar'',} \textit{COLT Seminar, Universitat Pompeu Fabra}, November 2023\\[3pt] 
{``Comparing humans and language models: reasoning \& grammar'',} \textit{UC San Diego Cognitive Science Seminar}, October 2023\\[3pt] 
{``Comparing humans and language models: challenges \& opportunities'',} \textit{International Interdisciplinary Computational Cognitive Science Summer School}, September 2023\\[3pt] 
{``Passive learning of active causal strategies'',} \textit{Max Planck Institute for Biological Cybernetics}, June 2023\\[3pt]
{``Passive learning of active causal strategies'',} \textit{Imperial College ICARL Seminar}, June 2023\\[3pt]
{``Comparing humans and language models: challenges \& opportunities'',} \textit{126th International Titisee Conference on NeuroAI}, March 2023\\[3pt] 
{``Comparing humans and language models: reasoning \& grammar'',} \textit{Carnegie Mellon University BrAIn Seminar}, February 2023\\[3pt] 
{``Language models show human-like content effects on reasoning'',} \textit{London Machine Learning Meetup}, November 2022\\[3pt] 
{``Augmenting reinforcement learning with language'',} \textit{University of Edinburgh Computational Cognitive Science Seminar}, November 2022\\[3pt] 
{``Compositionality (avoiding the question)'',} \textit{Brown University}, October 2022\\[3pt] 
{``Augmenting reinforcement learning with language'',} \textit{University of Tokyo, Matsuo Lab}, August 2022\\[3pt] 
{``Tell me why---Explanations improve learning of relational and causal structure'',} \textit{Spotlight Presentation, International Conference on Machine Learning}, July 2022\\[3pt] 
{``Tell me why---Explanations improve learning of relational and causal structure'',} \textit{NYU Concepts \& Categories Seminar}, February 2022\\[3pt] 
{``A computational framework for learning and transforming task representations'',} \textit{Cognitive Science Society Glushko Dissertation Prize Talk}, July 2021\\[3pt] 
{``Task relationships, task transformations, and analogies'',} \textit{Analogical Minds Seminar}, May 2021\\[3pt] 
{``Multi-task learning, transfer, and abstraction'',} \textit{Parallel Distributed Processing and the Emergence of an Understanding of Mind}, Princeton University, September 2018\\[3pt] 
{``The Jabberwocky: One-shot and few-shot learning of word embeddings'',} \textit{Meaning in Context Workshop}, Center for the Study of Language and Information,  Stanford University, September 2017 
 
%\vspace{1pt}\section{Presentations} \vspace{-15pt} \rule{\textwidth}{0.5pt} \\[3pt]
%{``Automated curricula through setter-solver interactions'',} \textit{8th International Conference on Learning Representations}, 2020 \\[3pt]
%{``Environmental drivers of systematicity and generalisation in a situated agent'',} \textit{8th International Conference on Learning Representations}, 2020 \\[3pt]
%{``Zero-shot task adaptation by homoiconic meta-mapping'',} \textit{Learning Transferable Skills Workshop, NeurIPS}, 2019 \\ [3pt] 
%{``An analytic theory of generalization dynamics and transfer learning in deep linear networks'',} Natural / Artificial Intelligence, Stanford Neurosciences Institute, 2018\\[3pt]
%{``An analytic theory of generalization dynamics and transfer learning in deep linear networks'',} Parallel Distributed Processing and the Emergence of an Understanding of Mind, Princeton University, 2018\\[3pt]
%{``Analogies emerge from learning dynamics in neural networks'',} 39th Annual Meeting of the Cognitive Science Society, 2017\\[3pt]
%{``Fast and sparse learning with compositional concept training'',} 15th Neural Computation and Psychology Workshop, 2016%\\[3pt]
%{``Cherenkov Radiation Based False Positive Detection for Rare Decays'',} Berkeley Undergraduate Physics Spring Poster Session, 2012

\vspace{1pt}\section{Teaching Experience} \vspace{-15pt} \rule{\textwidth}{0.5pt} \\[3pt]
{\bf Teaching Assistant,} Stanford University Department of Psychology, 6 courses between Fall 2016 and Winter 2019
\begin{itemize} \itemsep -2pt
  \item Planned and taught discussion sections for undergraduate statistics \& memory courses and graduate statistics \& research methods courses. \item Gave lectures on reinforcement learning and wrote and graded homeworks for graduate course on Neural Network Models of Cognition. \item Held office hours. \end{itemize}\vspace{-8pt}
{\bf Undergraduate Student Instructor,} UC Berkeley Mathematics, Spring, Fall 2013, \& Spring 2014 
\begin{itemize} \itemsep -2pt
  \item Planned and taught discussion sections. \item Held office hours. \item Wrote and graded quizzes and midterms. \end{itemize}\vspace{-8pt}
{\bf Teaching Assistant,} UC Berkeley Early Academic Outreach Program, June-July 2013
\begin{itemize} \itemsep -2pt
 \item Held office hours. \item Substitute taught classes. \end{itemize}

\vspace{1pt}\section{Other Work Experience} \vspace{-15pt} \rule{\textwidth}{0.5pt} \\[3pt]
{\bf Statistics Consultant,} Stanford University Department of Psychology, 2016-2017, 2019-2020
\begin{itemize} \itemsep -2pt
 \item Advised graduate students on technical aspects of data collection, analysis, and modeling. \end{itemize}
\vspace{1pt}\section{Service} \vspace{-15pt} \rule{\textwidth}{0.5pt} \\[3pt]
{\bf Action Editor:}
\begin{itemize} \itemsep -2pt
 \item Transactions on Machine Learning Research
\end{itemize}
{\bf Area Chair:}
\begin{itemize} \itemsep -2pt
 \item Neural Information Processing Systems 
 \item Conference on Computational Linguistics
\end{itemize}
{\bf Reviewer:} 
\begin{itemize} \itemsep -2pt
 \item Artificial Intelligence
 \item Nature
 \item Nature Human Behavior 
 \item Nature Machine Intelligence 
 \item Nature Neuroscience 
 \item Proceedings of the National Academy of Sciences
 \item Computational Linguistics
 \item Current Biology 
 \item Neural Information Processing Systems
 \item International Conference on Learning Representations 
 \item International Conference on Machine Learning
 \item Association for Computational Linguistics
 \item Cognitive Science Society
 \item Conference on the Mathematical Theory of Deep Neural Networks (DeepMath)
 \item Journal of Educational Psychology
 \end{itemize}
{\bf Mentoring \& other service:} 
\begin{itemize} \itemsep -2pt
 \item DeepMind Scholars Mentor
 \item Deep Indaba Mentor
 \item Team DE\&I Lead, 2023 
 \item Cientifico Latino Graduate Student Mentorship Initiative
 \item ICLR 2022 Co-Submitting Summer
\end{itemize}
%\vspace{1pt}\section{Technical Skills} \vspace{-15pt} \rule{\textwidth}{0.5pt} \\[3pt]
%{\bf Computer science:} Experienced with both theory and practice. 
%\begin{itemize} \itemsep -2pt
%  \item Graduate coursework in machine learning, neural networks, and probabilistic models \& algorithms.
%  \item Experienced user of Python, R, C++, JavaScript, Matlab, some knowledge of C, Mathematica, Macaulay2, Haskell. 
%  \item Used many common libraries for these languages, e.g. numpy, jax, scipy, tidyr, dplyr, jquery, matplotlib, FFTW.
%  \item Used many machine learning libraries, including TensorFlow, Torch, scikit-learn, and Caffe.
%  \item Experienced with *NIX operating systems.
%\end{itemize}\vspace{-8pt}
%{\bf Mathematics:} Knowledge across many domains, with applications.
%\begin{itemize} \itemsep -2pt
%\item Algebraic geometry, group theory, category theory, topology, etc. \item Practical applications to machine learning, computer vision, neural coding, etc. \end{itemize}\vspace{-8pt}
%{\bf Statistics:} Significant experience with standard data analysis techniques.
%\begin{itemize} \itemsep -2pt
%  \item Linear modeling, hierarchical modeling, etc.
%  \item Fitting algorithms \& goodness-of-fit tests. \end{itemize} \vspace{-8pt}
%{\bf Physics:} Experienced in a wide variety of applied and experimental contexts.\begin{itemize} \itemsep -2pt
%\item Statistical mechanics, biophysics, analytic mechanics, etc. \item Experimentation ranging from NMR to quantum entanglement. \end{itemize}\vspace{-8pt}
%{\bf Modeling \& Simulation:} Developed models and simulations of various phenomena. 
%\begin{itemize} \itemsep -2pt
%  \item Developed both from published methods and directly from physical principles. \end{itemize}
%{\bf Laboratory Equipment:} Competent with most common laboratory equipment. 
%\begin{itemize} \itemsep -2pt
% \item Oscilloscopes, standard \& lock-in amplifiers, signal generators, etc. \end{itemize} 
%%\vspace{-8pt}
%%{\bf Computer} Experienced with various operating systems and software applications.
%%\begin{itemize} \itemsep -2pt
%% \item Windows and Linux systems. \item Office, Photoshop \& Illustrator. \item  HTML \& \LaTeX.\end{itemize} 
\vspace{1pt}\section{Other Activities} \vspace{-15pt} \rule{\textwidth}{0.5pt} \\[3pt]
{\bf Carillon:} Carilloneur member of the Guild of Carilloneurs in North America (\url{www.gcna.org}). \\[3pt]
{\bf Rock climbing:} Bouldering, sport, and trad. Former routesetter at Stanford Climbing Wall, set problems for Collegiate Climbing Series events.

\end{resume}
\end{document} 



